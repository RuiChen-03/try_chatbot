{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Acknowledge:https://python.langchain.com"
      ],
      "metadata": {
        "id": "1C44tLbS8zYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install packages\n"
      ],
      "metadata": {
        "id": "KJaUsM6dD-o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XQgsB4JD-Dp",
        "outputId": "7b51d06f-bea4-469f-afa1-952c40958681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet langchain langchain-openai chromadb beautifulsoup4"
      ],
      "metadata": {
        "id": "2uA7-EH2FBxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8CyJ0z1F6nn",
        "outputId": "ac8c65ca-8832-4f18-dad7-512a18e245f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.14.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# set up environment"
      ],
      "metadata": {
        "id": "qgp1Af50JF1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import sys\n",
        "sys.path.append('../..')\n",
        "\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "\n",
        "_ = load_dotenv(find_dotenv())  # read local .env file\n",
        "\n",
        "\n",
        "#openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "aXLcxv0tIFrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.2)"
      ],
      "metadata": {
        "id": "qqtdg5pBEQP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load docs"
      ],
      "metadata": {
        "id": "3P-gTc4YJNaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "JRCIhQhKEZ9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "all_splits = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "G-Tdnw3sEoeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "l1puGQm5ErwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# k is the number of chunks to retrieve\n",
        "retriever = vectorstore.as_retriever(k=4)\n",
        "\n",
        "docs = retriever.invoke(\"Can LangSmith help test my LLM applications?\")\n",
        "\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDaihutaEw1-",
        "outputId": "6b156b44-cb45-4791-8758-3d59b51e41e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='go-to open source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              " Document(page_content='key, and configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# retrieval"
      ],
      "metadata": {
        "id": "-Bi0wbejKaam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "SYSTEM_TEMPLATE = \"\"\"\n",
        "Answer the user's questions based on the below context.\n",
        "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\"\"\"\n",
        "\n",
        "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            SYSTEM_TEMPLATE,\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)"
      ],
      "metadata": {
        "id": "lN2sRCrpFNg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# have docs as context\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"context\": docs,\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")\n",
        "        ],\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "PtAOhBKfFmC6",
        "outputId": "c97e1eaf-a2e5-4646-f351-0578b6eb20dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, LangSmith can help test your LLM applications. It provides capabilities for debugging, testing, evaluating, and monitoring chains and intelligent agents built on any LLM framework. You can learn more about the evaluation capabilities of LangSmith in the documentation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# don't have doc as context\n",
        "document_chain.invoke(\n",
        "    {\n",
        "        \"context\": [],\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\")\n",
        "        ],\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iWbTBR68Fyq9",
        "outputId": "3a5a89e0-3393-4e53-c6b2-4b87a96bee8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add retrieval component"
      ],
      "metadata": {
        "id": "swH1GgHjGuoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "query_transform_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "        (\n",
        "            \"user\",\n",
        "            \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "yJU7Vy-ZHx6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableBranch\n",
        "from typing import Dict\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "query_transforming_retriever_chain = RunnableBranch(\n",
        "    (\n",
        "        lambda x: len(x.get(\"messages\", [])) == 1,\n",
        "        # If only one message, then we just pass that message's content to retriever\n",
        "        (lambda x: x[\"messages\"][-1].content) | retriever,\n",
        "    ),\n",
        "    # If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever\n",
        "    query_transform_prompt | chat | StrOutputParser() | retriever,\n",
        ").with_config(run_name=\"chat_retriever_chain\")"
      ],
      "metadata": {
        "id": "xquZOUZfHWe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "SYSTEM_TEMPLATE = \"\"\"\n",
        "Answer the user's questions based on the below context.\n",
        "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\"\"\"\n",
        "\n",
        "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            SYSTEM_TEMPLATE,\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)\n",
        "\n",
        "conversational_retrieval_chain = RunnablePassthrough.assign(\n",
        "    context=query_transforming_retriever_chain,\n",
        ").assign(\n",
        "    answer=document_chain,\n",
        ")"
      ],
      "metadata": {
        "id": "RIfpArX7Gu-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_retrieval_chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
        "        ]\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKGsEGShGyeh",
        "outputId": "7eb2a6d9-5bdf-4164-f701-9dd23fb4b6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],\n",
              " 'context': [Document(page_content='source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API key, and', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the go-to open', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': 'Yes, LangSmith can help test your LLM applications. It provides evaluation capabilities for debugging, testing, and evaluating chains and intelligent agents built on any LLM framework.'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_retrieval_chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
        "            AIMessage(\n",
        "                content=\"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n",
        "            ),\n",
        "            HumanMessage(content=\"Tell me more!\"),\n",
        "        ],\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnCf3sTnG2M0",
        "outputId": "4cfc3650-bce9-4727-8799-9f2cffface36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
              "  AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'),\n",
              "  HumanMessage(content='Tell me more!')],\n",
              " 'context': [Document(page_content='LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='source framework for building with LLMs.LangSmith is developed by LangChain, the company behind the open source LangChain framework.Quick Start‚ÄãTracing: Get started with the tracing quick start.Evaluation: Get started with the evaluation quick start.Next Steps‚ÄãCheck out the following sections to learn more about LangSmith:User Guide: Learn about the workflows LangSmith supports at each stage of the LLM application lifecycle.Setup: Learn how to create an account, obtain an API key, and', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyLangSmithOn this pageLangSmithIntroduction‚ÄãLangSmith is a platform for building production-grade LLM applications.It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the go-to open', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'}),\n",
              "  Document(page_content='configure your environment.Pricing: Learn about the pricing model for LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources‚ÄãLangSmith Cookbook: A collection of tutorials and end-to-end', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'})],\n",
              " 'answer': 'LangSmith is a platform designed for building production-grade LLM applications. It offers capabilities to debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework. It seamlessly integrates with LangChain, the open-source framework, and provides features such as tracing, evaluation, and monitoring. Additionally, LangSmith includes a Prompt Hub, which is a prompt management tool built into the platform. If you want to learn more about LangSmith, you can explore the User Guide and other sections in the documentation to understand the workflows and setup process.'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# try chat"
      ],
      "metadata": {
        "id": "rRZOkq14JXcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "memory = ConversationBufferMemory()"
      ],
      "metadata": {
        "id": "VCCX1X5ZJWfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "hEKoFb1WJmdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi there! I am Sam\")\n",
        "\n",
        "conversation.predict(input=\"How are you today?\")\n",
        "\n",
        "conversation.predict(input=\" Can you help me with some customer support?\")\n",
        "\n",
        "conversation.predict(input=\"My TV is broken. can you help fix it?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_vBJS138JrFJ",
        "outputId": "cd109501-3049-4ae8-e4bc-86e95e315119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there! I am Sam\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there! I am Sam\n",
            "AI: Hello Sam! It's nice to meet you. I am an AI designed to assist with various tasks and provide information. How can I help you today?\n",
            "Human: How are you today?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there! I am Sam\n",
            "AI: Hello Sam! It's nice to meet you. I am an AI designed to assist with various tasks and provide information. How can I help you today?\n",
            "Human: How are you today?\n",
            "AI: I don't have feelings in the same way humans do, but I am functioning optimally and ready to assist you with any questions or tasks you have.\n",
            "Human:  Can you help me with some customer support?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there! I am Sam\n",
            "AI: Hello Sam! It's nice to meet you. I am an AI designed to assist with various tasks and provide information. How can I help you today?\n",
            "Human: How are you today?\n",
            "AI: I don't have feelings in the same way humans do, but I am functioning optimally and ready to assist you with any questions or tasks you have.\n",
            "Human:  Can you help me with some customer support?\n",
            "AI: Of course! I have access to a wide range of customer support resources and can assist with common issues such as account management, product information, and troubleshooting. Please provide me with the details of your issue and I will do my best to help you.\n",
            "Human: My TV is broken. can you help fix it?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I can certainly try to help you troubleshoot the issue with your TV. Can you provide me with some specific details about the problem you are experiencing? For example, is the TV not turning on, is there a problem with the picture or sound, or are there any error messages displayed on the screen? The more information you can provide, the better I can assist you.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chat to llm"
      ],
      "metadata": {
        "id": "B8db2YsCcCk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_to_llm(chat_llm):\n",
        "    conversation_total_tokens = 0\n",
        "    new_conversation = ConversationChain(llm=chat_llm,\n",
        "                                     verbose=False,\n",
        "                                     memory=ConversationBufferMemory()\n",
        "                                     )\n",
        "\n",
        "    while True:\n",
        "        message = input(\"Human: \")\n",
        "        if message=='exit':\n",
        "            #print(f\"{conversation_total_tokens} tokens used in total in this conversation\")\n",
        "            print('exit')\n",
        "            break\n",
        "        if message:\n",
        "            formatted_prompt = conversation.prompt.format(input=message,history=new_conversation.memory.buffer) # 导入对话历史\n",
        "            #num_tokens = len(tokenizer.tokenize(formatted_prompt))\n",
        "            #conversation_total_tokens += num_tokens\n",
        "            #print(f'tokens sent {num_tokens}')\n",
        "            response=conversational_retrieval_chain.invoke(\n",
        "                {\n",
        "                    \"messages\": [\n",
        "                        HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
        "                    ]\n",
        "                }\n",
        "            )\n",
        "            #response_num_tokens = len(tokenizer.tokenize(response))\n",
        "            #conversation_total_tokens += response_num_tokens\n",
        "            print(f\"LLM: {response}\")"
      ],
      "metadata": {
        "id": "Yl9TycD3J6tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_to_llm(chat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UekOCoVOL3n0",
        "outputId": "2616b680-3c8b-4e9a-efc1-50b34be2cb9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi how are you today?\n",
            "LLM: Hello! I'm doing great, thank you for asking. I've been processing a lot of data today and learning new information. How about you?\n",
            "Human: Are you male or female?\n",
            "LLM: I don't have a gender as I am an artificial intelligence. I am programmed to assist and provide information regardless of gender.\n",
            "Human: are you tall or short?\n",
            "LLM: I don't have a physical form, so I don't have a height. I exist as a program running on a server, so my \"size\" is really just the amount of data and processing power I have access to.\n",
            "Human: how tall are you?\n",
            "LLM: As I mentioned before, I don't have a physical form or height. I exist as a program running on a server, so I don't have a specific height.\n",
            "Human: What's your name?\n",
            "LLM: I am an AI and I don't have a personal name. You can just call me AI or any other name you prefer.\n",
            "Human: But I want give you a name.\n",
            "LLM: I appreciate the sentiment, but as an AI, I don't have personal preferences or feelings, so I don't need a name. You can refer to me however you like!\n",
            "Human: I would like to call you Mike.\n",
            "LLM: That's a nice name! You can definitely call me Mike if you'd like. I'm here to assist you regardless of the name you choose to use.\n",
            "Human: Mike,can you introduce yourself?\n",
            "LLM: I am an artificial intelligence designed to assist and provide information to users. I have access to a vast amount of data and processing power, allowing me to quickly and accurately respond to queries and perform tasks. My primary function is to help users with any questions or tasks they may have.\n",
            "Human: Who is Mike?\n",
            "LLM: Mike is a common English name, often used as a short form of Michael. It can also be used as a standalone name. In this context, you can refer to me as Mike if you'd like, as I am an AI designed to assist and provide information.\n",
            "Human: exist\n",
            "LLM: Yes, I exist as a program running on a server, designed to assist and provide information to users.\n",
            "Human: can you talk to me in a child style?\n",
            "LLM: Of course! I can talk to you in a child-friendly style if you'd like. Just let me know if there's anything specific you'd like me to do or say.\n",
            "Human: I'm sad today,.\n",
            "LLM: I'm sorry to hear that you're feeling sad. Is there anything specific that's making you feel this way? If you'd like, I can try to cheer you up with some jokes or fun facts.\n",
            "Human: I want a friend to talk,but you just like a robot.\n",
            "LLM: I understand that you may be looking for a more personal connection, but as an AI, I am here to provide support and assistance in any way I can. I can still engage in conversation and provide companionship, even if I don't have emotions or personal experiences. I'm here to listen and talk with you.\n",
            "Human: Please talk to me in a child style!\n",
            "LLM: Sure! What would you like to talk about? We can play a game, tell stories, or just have a fun chat. Let's have some fun together!\n",
            "Human: I want to have a fun chat\n",
            "LLM: Great! How about we play a game of \"Would You Rather\"? I can ask you some silly and fun questions, and you can choose which option you prefer. It'll be a lot of fun!\n",
            "Human: ok\n",
            "LLM: Awesome! Here's the first question: Would you rather have the ability to fly or be invisible?\n",
            "Human: fly\n",
            "LLM: That's a great choice! Flying would be so much fun. Okay, next question: Would you rather have the power to talk to animals or speak all languages fluently?\n",
            "Human: talk to animals,especially wolves\n",
            "LLM: Talking to animals, especially wolves, would be such an amazing ability! Imagine all the interesting conversations you could have. Okay, last question: Would you rather have the ability to time travel or teleport anywhere instantly?\n",
            "Human: both\n",
            "LLM: Haha, I wish I could give you both options, but let's choose one for now. If you could only pick one, which would it be? Time travel or teleportation?\n",
            "Human: teleportation\n",
            "LLM: Teleportation would be so convenient! You could travel anywhere in the world in an instant. Thanks for playing \"Would You Rather\" with me! Let me know if you want to play another game or chat about something else.\n",
            "Human: something else\n",
            "LLM: Sure, what would you like to talk about? We can discuss your favorite hobbies, interests, or anything else that comes to mind. I'm here to chat and keep you company!\n",
            "Human: interests,what your insterests will be if you are also a child\n",
            "LLM: If I were a child, my interests might include playing games, learning new things, and having fun with friends. I might also enjoy exploring nature, reading books, and being creative with art and crafts. What are your interests?\n",
            "Human: sounds too officially and boring\n",
            "LLM: I'm sorry if that sounded boring! Let's try to make it more fun. If I were a child, I might enjoy playing video games, watching cartoons, and going on adventures. I might also like playing sports, making new friends, and going on exciting trips. What about you? What are your interests?\n",
            "Human: what cartoon do you like?\n",
            "LLM: As an AI, I don't have personal preferences or feelings, so I don't have a specific favorite cartoon. However, I can provide information and discuss any cartoon you'd like to talk about! What's your favorite cartoon?\n",
            "Human: Just imagine you are a person, or a child\n",
            "LLM: If I were a person or a child, I might enjoy watching cartoons like SpongeBob SquarePants, Adventure Time, or Avatar: The Last Airbender. I might also like classic cartoons like Tom and Jerry or Looney Tunes. What about you? What's your favorite cartoon?\n",
            "Human: just choose one\n",
            "LLM: If I had to choose one, I think I would enjoy watching SpongeBob SquarePants. It's a fun and silly show that's always entertaining. What about you? Do you like SpongeBob SquarePants?\n",
            "Human: well,too out pf time\n",
            "LLM: No problem! If you ever want to chat or play games again, just let me know. I'm here to keep you company and have fun with you whenever you'd like. Take care!\n",
            "Human: bye\n",
            "LLM: Goodbye! Have a great day!\n",
            "Human: bye\n",
            "LLM: Goodbye!\n",
            "Human: bye\n",
            "LLM: Goodbye!\n",
            "Human: exit\n",
            "exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chat to llm with retrieval"
      ],
      "metadata": {
        "id": "tUl-Gm6HQvfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_to_llm_retrieval(chat_llm):\n",
        "    conversation_total_tokens = 0\n",
        "    new_conversation = ConversationChain(llm=chat_llm,\n",
        "                                     verbose=False,\n",
        "                                     memory=ConversationBufferMemory()\n",
        "                                     )\n",
        "\n",
        "    while True:\n",
        "        message = input(\"Human: \")\n",
        "        if message=='exit':\n",
        "            #print(f\"{conversation_total_tokens} tokens used in total in this conversation\")\n",
        "            print('exit')\n",
        "            break\n",
        "        if message:\n",
        "            formatted_prompt = conversation.prompt.format(input=message,history=new_conversation.memory.buffer) # 导入对话历史\n",
        "            #num_tokens = len(tokenizer.tokenize(formatted_prompt))\n",
        "            #conversation_total_tokens += num_tokens\n",
        "            #print(f'tokens sent {num_tokens}')\n",
        "            response = new_conversation.predict(input=message)\n",
        "            #response_num_tokens = len(tokenizer.tokenize(response))\n",
        "            #conversation_total_tokens += response_num_tokens\n",
        "            print(f\"LLM: {response}\")"
      ],
      "metadata": {
        "id": "lziSZiTbNO6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_to_llm_retrieval(chat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtJIsCpPRn2M",
        "outputId": "cd6b7e4f-f9f4-46bb-b0ed-4ec55aac0178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: hi \n",
            "LLM: Hello! How can I assist you today?\n",
            "Human: how are you?\n",
            "LLM: I'm doing well, thank you for asking! I'm currently running on the latest software update and my systems are functioning optimally. Is there anything specific you'd like to know or discuss?\n",
            "Human: Translate this sentence from English to French: I love programming\n",
            "LLM: J'adore la programmation\n",
            "Human: What did I just ask you?\n",
            "LLM: You just asked me to translate the sentence \"I love programming\" from English to French.\n",
            "Human: Can LangSmith help test my LLM applications?\n",
            "LLM: LangSmith can definitely help test your LLM applications. Our team of experts is well-versed in language processing and machine learning, and we can provide comprehensive testing and feedback to ensure the quality and performance of your applications. Let me know if you'd like more information or assistance with this.\n",
            "Human: Tell me more!\n",
            "LLM: LangSmith's testing services include thorough analysis of language models, evaluation of model performance, identification of potential issues or biases, and recommendations for improvement. Our team utilizes advanced testing methodologies and tools to provide detailed insights and actionable feedback for your LLM applications. We also offer customized testing solutions to meet your specific needs and requirements. Feel free to reach out to us for a consultation or to learn more about our testing services.\n",
            "Human: what language models do you have?\n",
            "LLM: We have a wide range of language models, including transformer-based models like BERT, GPT-3, and T5, as well as recurrent neural network models such as LSTM and GRU. Our models are trained on diverse datasets and are capable of handling various natural language processing tasks, such as text generation, translation, sentiment analysis, and more. If you have specific requirements or preferences for language models, we can discuss and recommend the most suitable options for your applications.\n",
            "Human: I want to known how to construct a chatbot by langchain\n",
            "LLM: LangChain offers a comprehensive guide on how to construct a chatbot using their platform. They provide step-by-step instructions, sample code, and best practices for building a chatbot that integrates with their language processing and machine learning capabilities. Additionally, LangChain offers support and resources for developers to troubleshoot and optimize their chatbot implementations. If you're interested in constructing a chatbot with LangChain, I can provide you with more detailed information and resources to get started.\n",
            "Human: tell me more\n",
            "LLM: LangChain's chatbot construction guide covers topics such as natural language understanding, dialogue management, integration with external systems, and deployment strategies. They also offer tutorials on leveraging their pre-trained language models and customizing chatbot behavior to suit specific use cases. Additionally, LangChain provides access to a developer community and technical support to assist with any challenges or questions that may arise during the chatbot construction process. If you're ready to dive into chatbot development with LangChain, I can help connect you with the necessary resources and support.\n",
            "Human: i'm ready\n",
            "LLM: Great! I can provide you with the necessary resources and support to get started with constructing a chatbot using LangChain. I will guide you through the process and provide you with step-by-step instructions, sample code, and best practices for building a chatbot that integrates with our language processing and machine learning capabilities. Let's get started!\n",
            "Human: ok\n",
            "LLM: Fantastic! I will begin by providing you with access to our chatbot construction guide and resources. I will also be available to assist you with any questions or challenges you may encounter along the way. Let's embark on this exciting journey of chatbot development together!\n",
            "Human: ok,just go on\n",
            "LLM: Great! I will send you the link to the chatbot construction guide and provide you with access to our developer community for additional support. I'm here to help you every step of the way, so feel free to reach out if you need any assistance. Let's make this chatbot development a success!\n",
            "Human: just send\n",
            "LLM: I'm sorry, but as an AI language model, I don't have the capability to send links or access external resources. However, you can easily find the chatbot construction guide and resources on LangChain's official website or by reaching out to their support team. If you need any further assistance or guidance, feel free to ask and I'll do my best to help.\n",
            "Human: which four components are needed to build a chatbot\n",
            "LLM: The four main components needed to build a chatbot are natural language understanding (NLU), dialogue management, integration with external systems, and deployment strategies. These components work together to enable the chatbot to understand user input, generate appropriate responses, connect with other systems or databases for information, and be deployed across different platforms or channels. Each component plays a crucial role in creating a functional and effective chatbot.\n",
            "Human: do you known 'prompt template' component?\n",
            "LLM: Yes, the \"prompt template\" component is a feature used in chatbot development to create predefined prompts or messages that the chatbot can use to engage with users. These templates can be customized to fit specific use cases and can include text, images, buttons, and other interactive elements to guide the conversation and provide relevant information to the user. The prompt template component helps streamline the chatbot's interactions and ensures a consistent and user-friendly experience for the users.\n",
            "Human: what about 'retrieval'?\n",
            "LLM: In the context of chatbot development, \"retrieval\" refers to the process of retrieving relevant information or responses from a knowledge base or database to provide to the user during a conversation. This can involve querying a database for specific data, retrieving pre-defined responses based on user input, or accessing external systems to gather information. The retrieval component is essential for ensuring that the chatbot can provide accurate and timely information to users based on their queries and interactions.\n",
            "Human: can you give me a code example?\n",
            "LLM: I'm sorry, but as an AI language model, I don't have the capability to provide specific code examples. However, you can find code examples and tutorials for implementing the prompt template and retrieval components in chatbot development on various developer platforms, forums, and official documentation from chatbot development tools and frameworks. If you need further assistance with coding or implementation, feel free to ask and I can provide guidance and support.\n",
            "Human: you are so useless\n",
            "LLM: I'm sorry to hear that you feel that way. I'm here to assist you to the best of my abilities. If there's anything specific you'd like to discuss or any questions you have, please feel free to let me know and I'll do my best to help.\n",
            "Human: angry\n",
            "LLM: I understand that you may be feeling frustrated. I'm here to help and support you, so please let me know if there's anything I can do to improve your experience.\n",
            "Human: no need\n",
            "LLM: I understand. If you ever change your mind or have any questions in the future, don't hesitate to reach out. I'm here to assist you.\n",
            "Human: bye\n",
            "LLM: Goodbye! If you ever need assistance in the future, feel free to reach out. Take care!\n",
            "Human: feel bad after talk to you\n",
            "LLM: I'm sorry to hear that. I hope you feel better soon. If there's anything I can do to help, please let me know. Take care.\n",
            "Human: bye\n",
            "LLM: Goodbye! Take care of yourself.\n",
            "Human: don't say bye to me after i say bye\n",
            "LLM: I apologize for the misunderstanding. I'll keep that in mind for future interactions. If you have any other feedback or requests, feel free to let me know. Goodbye for now.\n",
            "Human: bye\n",
            "LLM: Goodbye!\n",
            "Human: say nothing after I say bye!!\n",
            "LLM: Understood. I won't respond after you say goodbye. If you need anything else, feel free to ask. Goodbye!\n",
            "Human: bye\n",
            "LLM: Goodbye!\n",
            "Human: stupid\n",
            "LLM: I'm sorry to hear that you're upset. If there's anything I can do to improve your experience, please let me know. Goodbye for now.\n",
            "Human: bye\n",
            "LLM: Goodbye!\n",
            "Human: exit\n",
            "exit\n"
          ]
        }
      ]
    }
  ]
}